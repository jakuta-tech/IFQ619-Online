{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "5a3b8c1b-a2dc-4056-b19d-46f34999dded",
            "metadata": {
                "cell_name": "header_cell"
            },
            "source": [
                "<div style=\"background:#E9FFF6; color:#440404; padding:8px; border-radius: 4px; text-align: center; font-weight: 500;\">IFN619 - Data Analytics for Strategic Decision Makers (2024 Sem 1)</div>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b4657c87-4b4c-4b2f-8d3d-a16f14854cc1",
            "metadata": {
                "cell_name": "title_cell"
            },
            "source": [
                "# IFN619 :: C3 - Ethical considerations in data analytics\n",
                "\n",
                "#### IMPORTANT: Access this module in Canvas for videos that address the content below"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d05ec6e8",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## [1] Representation, measurement, and meaning\n",
                "\n",
                "<img src=\"https://images.newscientist.com/wp-content/uploads/2023/09/13124024/SEI_170680473.jpg?width=600\"></img>\n",
                "\n",
                "[Tom Gould, New Scientist (2023)](https://www.newscientist.com/article/2391858-tom-gauld-on-taking-time-to-smell-the-roses/)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7375864f",
            "metadata": {},
            "source": [
                "#### What do we represent, and how do we measure?\n",
                "\n",
                "A famous quote (with doubts about the source):\n",
                "\n",
                "[Wikipedia: Lies, damn lies, and statistics](https://en.wikipedia.org/wiki/Lies,_damned_lies,_and_statistics)\n",
                "\n",
                "How we represent impacts how we make meaning:\n",
                "\n",
                "[Lies, damn lies, and statistics](https://www.infoworld.com/article/3088166/why-how-to-lie-with-statistics-did-us-a-disservice.html) - Andy Cotgreave, InfoWorld, 2016\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "35eaa304",
            "metadata": {},
            "source": [
                "<img src=\"https://images.techhive.com/images/article/2016/06/iraq-100668103-large.idge.png\" width=\"250px\"></img>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a84a8047",
            "metadata": {},
            "source": [
                "#### Change in the impact of data\n",
                "\n",
                "[A very short history of big data](https://whatsthebigdata.com/a-very-short-history-of-big-data/)\n",
                "\n",
                "[How Much Information Is There In the World?](https://www.lesk.com/mlesk/ksg97/ksg.html) - Michael Lesk (1997)\n",
                "\n",
                "- A few thousand Petabytes by 2000\n",
                "- no information will have to be thrown out,\n",
                "- the typical piece of information will never be looked at by a human being\n",
                "\n",
                "[Reprint: How Much Information?](https://quod.lib.umich.edu/j/jep/3336451.0006.204/--reprint-how-much-information?rgn=main;view=fulltext) - Peter Lyman & Hal R Varian (2000)\n",
                "\n",
                "- The world produces between 1 and 2 Exabytes ($10^{18}$) per year\n",
                "\n",
                "[The Expanding Digital Universe: A Forecast of Worldwide Information Growth Through 2010](http://www.tobb.org.tr/BilgiHizmetleri/Documents/Raporlar/Expanding_Digital_Universe_IDC_WhitePaper_022507.pdf) - John F Gantz et. al. (2007)\n",
                "\n",
                "- In 2006, 161 EB per year - about 3 million times all of the books ever written\n",
                "- By 2010, 988 EB per year\n",
                "\n",
                "[IDC: Data Creation Hit 64ZB in 2020](https://rcpmag.com/blogs/scott-bekker/2021/03/data-creation-hit-64zb-in-2020.aspx) - Scott Becker, Redmond Channel Partner (2021)\n",
                "\n",
                "- In 2020, 64 Zettabytes ($10^{21}$) per year\n",
                "\n",
                "> \"In 2020, 64.2ZB of data was created or replicated, defying the systemic downward pressure asserted by the COVID-19 pandemic on many industries and its impact will be felt for several years,\" said Dave Reinsel, senior vice president, IDC's Global DataSphere, in a statement Wednesday. \"The amount of digital data created over the next five years will be greater than twice the amount of data created since the advent of digital storage.\"\n",
                "\n",
                "[Wikipedia: Byte](https://en.wikipedia.org/wiki/Byte#Multiple-byte_units)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb310a6a",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "**READING 1:** [Personality detection from likes](https://www.pnas.org/doi/pdf/10.1073/pnas.1418680112) - Youyou, et. al. (2015)\n",
                "\n",
                "---\n",
                "\n",
                "\n",
                ">Youyou, W., Kosinski, M., & Stillwell, D. (2015). Computer-based personality judgments are more accurate than those made by humans. Proceedings of the National Academy of Sciences, 112(4), 1036–1040. https://doi.org/10.1073/pnas.1418680112\n",
                "\n",
                "<img src=\"https://www.pnas.org/cms/10.1073/pnas.1418680112/asset/f0dbcd53-7d2f-4b04-b1df-0ad043264f84/assets/graphic/pnas.1418680112fig01.jpeg\" width=\"600px\"></img>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75ab6e32",
            "metadata": {},
            "source": [
                "[All models are wrong](https://en.wikipedia.org/wiki/All_models_are_wrong#cite_note-2)\n",
                "\n",
                "> Box, George E. P. (1976), \"Science and statistics\" (PDF), Journal of the American Statistical Association, 71 (356): 791–799, doi:10.1080/01621459.1976.10480949.\n",
                "\n",
                "All models are wrong, but some are useful - Box (1978)\n",
                "\n",
                ">Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = nRT relating pressure P, volume V and temperature T of an \"ideal\" gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules. For such a model there is no need to ask the question \"Is the model true?\". If \"truth\" is to be the \"whole truth\" the answer must be \"No\". The only question of interest is \"Is the model illuminating and useful?\"."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "165d959f",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "**READING 2:** [Why Data Is Never Raw: On the seductive myth of information free of human judgment](https://www.thenewatlantis.com/publications/why-data-is-never-raw) - Nick Barrowman, The New Atlantis, 2018\n",
                "\n",
                "---\n",
                "\n",
                "**READING 3:** [The imitation game](https://academic.oup.com/mind/article/LIX/236/433/986238) - Alan Turing, Mind (1950)\n",
                "\n",
                "---\n",
                "\n",
                ">A. M. TURING, I.—COMPUTING MACHINERY AND INTELLIGENCE, Mind, Volume LIX, Issue 236, October 1950, Pages 433–460, https://doi.org/10.1093/mind/LIX.236.433"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "59f082f5",
            "metadata": {},
            "source": [
                "#### What is the significance in an age of AI?\n",
                "\n",
                "Turing's paper has been very influential on AI including the idea of testing intelligence with what is commonly refered to as the *Turing test*. However, there are critics of Turing's view on machine intelligence. In his book \"The Myth of Artificial Intelligence\", Erik J Larson writes (2021, p27):\n",
                "\n",
                "> \"AI's tone-deafness on social or situational intelligence has been noted before, more recently by machine learning scientist Francois Chollet, who summarizes his critique of Turing's (and, more broadly, the AI field's) view of intelligence nicely. First, intelligence is *situational* -- there is no such thing as general intelligence. Your brain is one piece in a broader system which includes your body, your environment, other humans, and culture as a whole. Second, it is *contextual* -- far from existing in a vaccuum, any individual intelligence will always be both defined and limited by its environment. (And currently, the environment, not the brain, is acting as the bottleneck to intelligence.) Third, human intelligence is largely *externalized*, constrained not in your brain but in your civilization. Think of individuals as toos, whose brains are modules in a cognitive system much larger than themselves -- a system that is self-improving and has been for a long time.\" \n",
                "\n",
                "**Note:** Chollet quote originally from Medium article \"The Implausibility of Intelligence Explosion\" (2017).\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e8b9f655",
            "metadata": {},
            "source": [
                "#### What if there is no empathy?\n",
                "\n",
                "[‘Empathetic’ AI has more to do with psychopathy than emotional intelligence – but that doesn’t mean we can treat machines cruelly](https://theconversation.com/empathetic-ai-has-more-to-do-with-psychopathy-than-emotional-intelligence-but-that-doesnt-mean-we-can-treat-machines-cruelly-225216#:~:text=Artificial%20systems%20hence%20show%20similarities,this%20ability%20for%20manipulative%20purposes.) - Catrin Misselhorn, The Conversation (2024)\n",
                "\n",
                "\n",
                "<img src=\"https://images.theconversation.com/files/582548/original/file-20240318-18-1voyly.jpg\" width=\"250px\"></img>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c3e6fced",
            "metadata": {},
            "source": [
                "[Will Artificial Intelligence Produce Synthetic Psychopaths?](https://www.forbes.com/sites/robertzafft/2021/02/14/will-artificial-intelligence-produce-synthetic-sociopaths/?sh=1bfd856b7d6e) - Robert Zafft (Forbes, 2021)\n",
                "\n",
                ">Some day soon, computers may convincingly mimic human empathy. Will this lead to ethical artificial intelligence, or synthetic psychopathy?\n",
                "\n",
                "<img src=\"https://imageio.forbes.com/specials-images/imageserve/6029a751dfccae5aa987e466/Ethical-artificial-intelligence--or-synthetic-psychiopathy/0x0.jpg?format=jpg&crop=3270,2688,x821,y325,safe&width=1440\" width=\"200px\"></img>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2ced37fd",
            "metadata": {},
            "source": [
                "**Warning: the following article contains a brief description of sexual violence**\n",
                "\n",
                "[Artificial Psychopathy](https://medium.com/predict/artificial-psychopathy-a10647d59cfb) - Stefano Diana (Medium, 2023)\n",
                "\n",
                ">The way psychopaths mimic emotional language without experiencing emotions is similar to how LLMs do it. With one difference: it’s not AI’s that take advantage of those skills to deceive people, but their masters.\n",
                "\n",
                "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GKek1SvnWdFI8RQX\" width=\"200px\"></img>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "11292462",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## [2] Issues to care about"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "78472998",
            "metadata": {},
            "source": [
                "### FAIRNESS"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "af7ad6ce",
            "metadata": {},
            "source": [
                "[Wikipedia: Facebook–Cambridge Analytica data scandal](https://en.wikipedia.org/wiki/Facebook–Cambridge_Analytica_data_scandal)\n",
                "\n",
                "[To work for society, data scientists need a hippocratic oath with teeth](https://www.wired.com/story/data-ai-ethics-hippocratic-oath-cathy-o-neil-weapons-of-math-destruction) Tom Upchurch, Wired, (2018)\n",
                "\n",
                "> This new bureaucracy is increasingly deciding who gets a job, who gets credit (and at what rate), who goes to prison and what information people read. Some of these systems may be making accurate decisions. However, Cathy argues that accuracy and efficiency alone are not sufficient metrics for success. Fairness, equity and other social considerations need to be built into the algorithms. Until this is done, these secretive and unaudited algorithms will continue to make unfair, bias and discriminatory decisions, on a systemic level."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "869a4662",
            "metadata": {},
            "source": [
                "[What does a fair algorithm look like?](https://www.wired.com/story/what-does-a-fair-algorithm-look-like/) - Luise Matsakis, Wired (2010)\n",
                "\n",
                "<img src=\"https://media.wired.com/photos/5bb69d549bfb5e2cd2cb3ac9/master/w_1920,c_limit/ai_bias_recourse_final.jpg\" width=\"400px\"></img>\n",
                "\n",
                "- Explanations\n",
                "- Recourse\n",
                "- Which data can be used\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8e1d22ce",
            "metadata": {},
            "source": [
                "#### How far have we come? Aren't things improving with GenAI?\n",
                "\n",
                "---\n",
                "\n",
                "**READING 4:** [Dialect prejudice predicts AI decisions about people’s character, employability, and criminality](http://arxiv.org/pdf/2403.00742) - Hofmann et al, Arxiv (2024)\n",
                "\n",
                "---\n",
                "\n",
                "- Overt racism improving\n",
                "- Covert racism worsening"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39151462",
            "metadata": {},
            "source": [
                "### ALILGNMENT & ACCOUNTABILITY "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "94ab8f0f",
            "metadata": {},
            "source": [
                "[Atlas of AI](https://www.amazon.com/Atlas-AI-Planetary-Artificial-Intelligence/dp/0300264631/ref=sr_1_1?crid=CXSUQXR86N7B&dib=eyJ2IjoiMSJ9.fbLbAdEWpZ0fy6hY9euyzSbKkB3YmNmGoM8H1LMGiTRwzmJJe-6nqjZBU1oYjPYeVwFaxExi9HAEcIkTKWP7GnT1Smk24GoHQOdQ9WjbieHRD909VsZpfhY0l36nG1YLuIUdODbMrl9xYlgIxMHbkJMhJvoENHVN84ZVEPm4emvmDbZUZCWFqPAGyMfo0SlWVLg5hz1wTcQDlROw-I751Y95Jk4Y2fFxW21NBpKblNw.KysWrsl0uE4uNDHGCXGiFtNudGCN7iHcICBZSqK2Kqg&dib_tag=se&keywords=atlas+of+AI&qid=1714872292&sprefix=atlas+of+ai%2Caps%2C276&sr=8-1) - Kate Crawford (2021)\n",
                "\n",
                "**Limits of measurement**\n",
                "\n",
                "> \"Making these choices about which information feeds AI systems to produce new classifications is a powerful moment of decision making: but who gets to choose and on what basis? The problem for computer science is that justice in AI systems will never be something that can be coded or computed. It requries a shift to assessing systems beyond optimization metrics and statistical parity and an understanding of where the frameworks of mathematics and engineering are causing the problems. This also means understanding how AI systems interact with data, workers, the environment, and the individuals whose lives will be affected by its use and deciding where AI should not be used.\" (p147-148)\n",
                "\n",
                "<img src=\"https://m.media-amazon.com/images/I/61Xe0C3QBOL._SL1500_.jpg\" width=\"100px\"></img>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fe1d2622",
            "metadata": {},
            "source": [
                "[What Does It Mean to Align AI With Human Values?](https://www.quantamagazine.org/what-does-it-mean-to-align-ai-with-human-values-20221213) - Melanie Mitchell, Qanta Magazine (2022)\n",
                "\n",
                "<img src=\"https://d2r55xnwy6nx47.cloudfront.net/uploads/2021/12/Mitchell-Melanie.jpg\" width=\"100px\"></img>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "03125779",
            "metadata": {},
            "source": [
                "### TRUST & TRANSPARENCY"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8a7da297",
            "metadata": {},
            "source": [
                "[The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do](https://www.amazon.com/Myth-Artificial-Intelligence-Computers-Think/dp/0674278666/ref=sr_1_1) - Erik J Larson (2021)\n",
                "\n",
                "**Trust as recognition of limits to inductive systems**\n",
                "\n",
                ">\"Thus limits to inductive AI lacking genuine understanding are increasingly pushed into AI discussion because we are rushing machines into service, in important areas of human life, which have no understanding ... Russell points to the \"alignment\" problem, an issue in AI of suddenly central importance, concerned with aligning current and future AI systems with our own interests and purposes. But the problem arises not, as Russell suggests, because AI systems are getting so smart so quickly, but rather because we've rushed them into positions of authority in so many areas of human society, and their inherent limitations -- which they've always had -- now matter.\" (p179)\n",
                "\n",
                "<img src=\"https://m.media-amazon.com/images/I/618bU2b3EVL._SL1000_.jpg\" width=\"100px\"></img>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c8a4eb4d",
            "metadata": {},
            "source": [
                "[What does a fair algorithm look like?](https://www.wired.com/story/what-does-a-fair-algorithm-look-like/) - Luise Matsakis, Wired (2010)\n",
                "\n",
                "> \"It’s necessary to ask: Transparent to whom and for what purpose? Transparency for the sake of transparency is not enough.\" ... \"For example, an algorithm that calculates loan approvals should explain not only why you were denied credit, but also what you can do to reverse the decision. It should say that you were denied the loan for having too little in savings, and provide the minimum amount you would need to additionally save to be approved. Offering counterfactual explanations doesn’t require the researchers who designed an algorithm release the code that runs it. That’s because you don’t necessarily need to understand how a machine learning system works to know why it reached a certain decision.\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25e9790f",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## [3] Principles to keep in mind"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eb48dba1",
            "metadata": {},
            "source": [
                "Drawing on 3 of Don Norman's principles of human centred design.\n",
                "https://jnd.org/the-four-fundamental-principles-ofhuman-centered-design-and-application/\n",
                "\n",
                "1. \"**Understand and Address the Core Problems.** Solve the fundamental, underlying issues, not the symptoms.\"\n",
                "2. \"**Be People-Centered.** Much of today’s systems, procedures, and devices are technology-centered, designed around the capabilities of the technology with people being asked to fill in the parts that the technology cannot do. People-centered means changing this, starting with the needs and abilities of people. It means considering all the people who are involved, taking account of the history, culture, beliefs, and environment of the community.\"\n",
                "3. \"**Use an Activity-Centered Systems Approach.** Design must focus upon the entire activity under consideration, not just isolated components. Moreover, activities do not exist in isolation: They are components of complex sociotechnical systems. Fixing or improving a small, local issue is often beneficial, but local optimization can result in sub-optimal global results. Focusing upon support of the activities is more important than optimization of the individual components.\" \n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4c064002",
            "metadata": {},
            "source": [
                "### My responsibility (understand and address core problems)\n",
                "\n",
                "not just surface problems\n",
                "\n",
                "not technology or capability driven\n",
                "\n",
                "**Avoiding assumptions**\n",
                "[Challenging Dangerous Assumptions by Andy Cohen](https://youtu.be/So89DJ1Rckc)\n",
                "\n",
                "**Awareness of own perspectives**\n",
                "\n",
                "---\n",
                "\n",
                "**READING 5:** [20 lessons on bias in machine learning systems from NIPS 2017 Keynote](https://hub.packtpub.com/20-lessons-bias-machine-learning-systems-nips-2017/)\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8ff75ad7",
            "metadata": {},
            "source": [
                "### My focus (people centred)\n",
                "\n",
                "**Understanding algorithm impact**\n",
                "\n",
                "[Facebook translates 'good morning' into 'attack them', leading to arrest](https://www.theguardian.com/technology/2017/oct/24/facebook-palestine-israel-translates-good-morning-attack-them-arrest) - Alex Hern, The Guardian (2017)\n",
                "\n",
                "\n",
                "[THE FLAWED ALGORITHM AT THE HEART OF ROBODEBT](https://pursuit.unimelb.edu.au/articles/the-flawed-algorithm-at-the-heart-of-robodebt) - Murray, Cheong & Patterson (2023)\n",
                "\n",
                "> Robodebt teaches us that even simple automated decision-making systems come with the biases of the people, systems and policies that conceive them\n",
                "\n",
                "<img src=\"https://res-2.cloudinary.com/the-university-of-melbourne/image/upload/s--vIjf9_Rl--/c_limit,f_auto,q_75,w_1784/v1/pursuit-uploads/452/15d/5fd/45215d5fd50419a39e2ef1b742a9a57ac6ae2486a2a496467ef288bb3b3b.jpg\" width=\"400px\"></img>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9816875",
            "metadata": {},
            "source": [
                "**Understanding mathwashing**\n",
                "\n",
                "[What is Math washing?](http://www.mathwashing.com)\n",
                "\n",
                "2 types:\n",
                "1. Acidental\n",
                "2. On purpose\n",
                "\n",
                "2 things to know:\n",
                "1. People design algorithms\n",
                "2. Data is not automatically objective\n",
                "\n",
                "3 things to do:\n",
                "1. Demand algorithmic transparency\n",
                "2. View algorithms as a kind of law\n",
                "3. Think critically"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "68b4c449",
            "metadata": {},
            "source": [
                "**Accounting for differences in understanding**\n",
                "\n",
                "[People Have Very Different Understandings of Even the Simplest Words](https://www.scientificamerican.com/article/people-have-very-different-understandings-of-even-the-simplest-words/) - Simon Makin (2024)\n",
                "\n",
                "> He [Kris De Meyer] runs workshops designed to show economists and climate scientists that they think very differently about words such as risk and uncertainty. \"They have these different concepts of terms they think they all understand but actually don’t,\" De Meyer says. \"They don’t understand each other because they have very different practices, which leads to different semantic representations in the brain.\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73f1b89e",
            "metadata": {},
            "source": [
                "### My actions (activity centred systems approach)\n",
                "\n",
                "#### Understanding and mitigating bias\n",
                "\n",
                "[Bias in Computer Systems](https://nissenbaum.tech.cornell.edu/papers/Bias%20in%20Computer%20Systems.pdf) - Friedman & Nissenbaum, ACM Transactions on Information Systems (1996)\n",
                "\n",
                ">\"we use the term bias to refer to computer systems that systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others.\" (p332)\n",
                "\n",
                "1. Pre-existing Bias\n",
                "    * Individual\n",
                "    * Societal\n",
                "2. Technical Bias\n",
                "    * Computer tools\n",
                "    * Decontextualized Algorithms\n",
                "    * Random number generation\n",
                "    * Formalization of Human Constructs\n",
                "3. Emergent Bias\n",
                "    * New Societal Knowledge\n",
                "    * Mismatch between users and system design\n",
                "        * Different expertise\n",
                "        * Different values\n",
                "\n",
                "[Algorithmic Accountability: A Primer](https://apo.org.au/sites/default/files/resource-files/2018-04/apo-nid142131.pdf)\n",
                "\n",
                "1. Fairness and Bias\n",
                "2. Opacity and Transparency\n",
                "3. Repurposing Data and Repurposing Algorithms\n",
                "4. Lack of Standards for Auditing\n",
                "5. Power and Control\n",
                "6. Trust and Expertise"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb3147b1",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "**READING 6:** [The Problem with Metrics is a Fundamental Problem for AI](https://arxiv.org/abs/2002.08512) - Rachel L Thomas & David Uminsky, Arxiv (2020)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2aa7e085",
            "metadata": {},
            "source": [
                "#### Understanding social context and legal requirements\n",
                "\n",
                "**Privacy and Data Soveriegnty in Australia**\n",
                "\n",
                "[Influence of international digital platforms](https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Economics/Digitalplatforms/Report) - Report, November 2023\n",
                "\n",
                "\n",
                "**EU General Data Protection Regulation (GDPR)**\n",
                "\n",
                "[How GDPR Drives Real-Time Analytics](https://datafloq.com/read/gdpr-drives-real-time-analytics/4824) - Ronald van Loon (2018)\n",
                "\n",
                "There are six underlying principles of GDPR.\n",
                "\n",
                "1. Organizations must ensure that the personal data of users is processed transparently, lawfully, and fairly.\n",
                "2. Personal data of users must only be collected for explicitly specified and legitimate purposes.\n",
                "3. Data collectors must only gather limited amounts of personal information that is adequate to the gatherer’s needs and relevant to their business.\n",
                "4. It is the responsibility of data collectors to ensure that the personal data is accurate and kept up to date.\n",
                "5. Data collectors must maintain personal data in a form where the data subject can be identified for only as long as it is necessary for processing.\n",
                "6. Personal data must be processed in a way that ensures that it remains secure and cannot be stolen."
            ]
        }
    ],
    "metadata": {
        "creation_period": "",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        },
        "nb_name": "template",
        "qut": {
            "creation_period": "2023_sem1",
            "nb_name": "C3-Ethics",
            "unit_code": "IFN619"
        },
        "unit_code": ""
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
