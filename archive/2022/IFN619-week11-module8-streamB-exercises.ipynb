{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6aI01kSh_7f"
   },
   "source": [
    "# IFN619. Week11: Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hfs36QT-h94f"
   },
   "source": [
    "In this tutorial, you will implement predictive models to determine house prices and for textual data. You will also apply LIME to gain insights about your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vE6KOQZJ1qMu"
   },
   "source": [
    "## Prediction using Regression Models: The Boston House Dataset\n",
    "\n",
    "This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms. However, these comparisons were primarily done outside of Delve and are thus somewhat suspect. The dataset is small in size with only 506 cases.\n",
    "\n",
    "The data was originally published by Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 14 attributes in each case of the dataset. They are:\n",
    "\n",
    "\n",
    "| Feature | Description                                                         |\n",
    "|---------|---------------------------------------------------------------------|\n",
    "| CRIM    | per capita crime rate by town                                       |\n",
    "| ZN      | proportion of residential land zoned for lots over 25,000 sq.ft.    |\n",
    "| INDUS   | proportion of non-retail business acres per town                    |\n",
    "| CHAS    | Charles River dummy variable (1 if tract bounds river; 0 otherwise) |\n",
    "| NOX     | nitric oxides concentration (parts per 10 million)                  |\n",
    "| RM      | average number of rooms per dwelling                                |\n",
    "| AGE     | proportion of owner-occupied units built prior to 1940              |\n",
    "| DIS     | weighted distances to five Boston employment centres                |\n",
    "| RAD     | index of accessibility to radial highways                           |\n",
    "| TAX     | fullvalue property tax rate per USD 10,000                          |\n",
    "| PTRATIO | pupil/teacher ratio by town                                         |\n",
    "| B       | proportion of blacks by town                                        |\n",
    "| LSTAT   | percentage of lower status of the population                        |\n",
    "| MEDV    | Median value of owner-occupied homes in USD 1000's                  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to apply a model that predits house prices and to inspect the predictionsusing LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler,MinMaxScaler\n",
    "\n",
    "# keras / deep learning libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T00:11:22.001315Z",
     "start_time": "2020-05-06T00:11:21.930901Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xm5Zlo8LCOYZ"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv(\"data/HousingData.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are missing values in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kyy3N1iXHLC9"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill up the missing values of a feature with the average of that feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# variable Age is an integer, make sure that you convert this value to an integer\n",
    "# your code goes here\n",
    "\n",
    "\n",
    "# rename column MEDV to PRICE (the variable that we want to predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the variables that are relevant for our prediction: PRICE.\n",
    "\n",
    "Look at the correlation matrix and choose which variables should we keep for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "correlation_matrix = \n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix: \n",
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard the feature that have an absolute correlation value with PRICE < 0.3\n",
    "# your code goes here\n",
    "data_selection = \n",
    "data_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate your data into features and class variable\n",
    "# your code goes here\n",
    "\n",
    "feature_names = \n",
    "\n",
    "class_var = \n",
    "\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise your data vales in X. Use the MinMax Scaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate your data into training (70%), validation (15%) and testset (15%)\n",
    "# your code goes here\n",
    "X_train, X_test, Y_train, Y_test = \n",
    "X_val, X_test, Y_val, Y_test = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a predictive model\n",
    "\n",
    "In this unit, we focused on Neural Networks, however there are plenty of models that we can use.\n",
    "In this tutorial, we will present two types of predictive models for regression: XGBoost (based on trees and bosting theory, and neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "In your studio sessions, we presented many neural networks, but for classification. \n",
    "For regression, the model is very similar. The difference is that we need to adjust the loss function and choose, for instance, the mean squared error. Let's see how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = Sequential()\n",
    "\n",
    "model_nn.add(Dense(10, input_dim = X_train.shape[1], activation = \"tanh\"))\n",
    "model_nn.add(Dense(7, activation = \"relu\"))\n",
    "model_nn.add(Dense(3, activation = \"relu\"))\n",
    "model_nn.add(Dense(1, activation = \"relu\"))\n",
    "\n",
    "# selecting the loss function with mean squared error \n",
    "model_nn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model  (this will take a while)\n",
    "history = model_nn.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=400, \n",
    "                       batch_size = 64, callbacks = callbacks_list, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history.history['mean_squared_error'], label='train')\n",
    "pyplot.plot(history.history['val_mean_squared_error'], label='val')\n",
    "pyplot.ylabel('mean_squared_error', fontsize=12)\n",
    "pyplot.xlabel('iterations', fontsize=12)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model (remember in regression: the closer to zero, the better)\n",
    "_, train_mse_nn = model_nn.evaluate(X_train, Y_train, verbose=0)\n",
    "_, test_mse_nn = model_nn.evaluate(X_test, Y_test, verbose=0)\n",
    "_, val_mse_nn = model_nn.evaluate(X_val, Y_val, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f, Validation: %.3f' % (train_mse_nn, test_mse_nn, val_mse_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing Mean Squared Error\n",
    "preds = model_nn.predict(X_test)\n",
    "nn_mse = np.sqrt(mean_squared_error(Y_test, preds))\n",
    "print(\"Mean Squared Error: \" + str(nn_mse))\n",
    "# computing r^2 score\n",
    "nn_r2 =  r2_score(Y_test, preds)\n",
    "print(\"R^2 Error: \" + str(nn_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "XGBoost combines Trees with Boosting theory. \n",
    "\n",
    "Boosting:\n",
    "Kearns and Valiant were the first to pose the question of whether a \"weak\" learning algorithm that performs just slightly better than random guessing can be \"boosted\" into an arbitrarily accurate \"strong\". The general idea of boosing is to improve a single weak model by combining it with a number of other weak models in order to generate a collectively strong model. In other words, to generate several classifiers in a sequence and focus on the training examples that were misclassified to build a new model that pays a \"higher attention\" to those \"weak learners\". The general output, is an average of the predictions of all classifiers generated during the training process. For the case of XGBoost, these classifiers are trees. At each iteration, a tree is learned. The training examples that were misclassified by a tree, go the next iteration with higher weights so the new classifier can find rules to correctly classify these \"week learners\". The final prediction is the average of the predictions of all trees trained (the ensemble model). XGBoost is a black-box, because the predictions are computed from an average weight of different trees (more info: https://medium.com/geekculture/xgboost-versus-random-forest-898e42870f30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/publication/348025909/figure/fig2/AS:1020217916416002@1620250314481/Simplified-structure-of-XGBoost.ppm\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "import xgboost as xgb\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=101,\n",
    "    n_estimators=1000,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=300,\n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    eval_set=[(X_val, Y_val)],\n",
    "    verbose=False,  # Disable logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing Mean Squared Error\n",
    "preds = model_xgb.predict(X_test)\n",
    "xbg_mse = np.sqrt(mean_squared_error(Y_test, preds))\n",
    "print(\"Mean Squared Error: \" + str(xbg_mse))\n",
    "# computing r^2 score\n",
    "xgb_r2 =  r2_score(Y_test, preds)\n",
    "print(\"R^2 Error: \" + str(xgb_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also check the feature importance for XGBoost\n",
    "from xgboost import plot_importance\n",
    "\n",
    "booster = model_xgb.get_booster()\n",
    "\n",
    "# plot feature importance\n",
    "# Get the importance dictionary (by gain) from the booster\n",
    "importance = booster.get_score(importance_type=\"gain\")\n",
    "\n",
    "# make your changes\n",
    "for key in importance.keys():\n",
    "    importance[key] = round(importance[key],2)\n",
    "\n",
    "# provide the importance dictionary to the plotting function\n",
    "ax = plot_importance(importance, max_num_features=5, importance_type='gain', show_values=True)\n",
    "\n",
    "# print the feature names\n",
    "print(\"f4: \" + X.columns.tolist()[4], \n",
    "      \"f10: \" + X.columns.tolist()[10], \n",
    "      \"f3: \" + X.columns.tolist()[3], \n",
    "      \"f8: \" + X.columns.tolist()[8],\n",
    "      \"f7:\" + X.columns.tolist()[7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating explanations with LIME\n",
    "\n",
    "Choose different instances of the test set and generate LIME Explanations for both the NN model and the XGB model. Discuss the explanations generated for the different models.\n",
    "\n",
    "For the XGB model, compare the explanations generated with the model's overall feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating explanations for Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a datapoint from the testset\n",
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# explain instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating explanations for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a datapoint from the testset\n",
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# explain instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "UncertaintyInFlowerDataTutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
