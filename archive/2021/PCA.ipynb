{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update for tensorflow\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random as rn\n",
    "import re\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "# Force TensorFlow to single thread\n",
    "# Multiple threads are a potential source of non-reprocible research resulsts\n",
    "session_conf = tf.compat.v1.ConfigProto( intra_op_parallelism_threads=1, inter_op_parallelism_threads=1 )\n",
    "\n",
    "# tf.set_random_seed() will make random number generation in the TensorFlow backend\n",
    "# have a well defined initial state\n",
    "# more details: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.compat.v1.set_random_seed(515)\n",
    "\n",
    "# keras / deep learning libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential # a sequence of neuronal layers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense # layer representing a neuron\n",
    "from tensorflow.keras.optimizers import Nadam # optimisation algorithm to find the best weights in the model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.image as mpimg\n",
    "import pylab as pl\n",
    "from pylab import savefig\n",
    "plt.style.use('seaborn-deep')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Figure Plotting libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# mlp for the two circles classification problem\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot results\n",
    "\n",
    "def myplot(score,coeff, y, labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley, c = y)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "            \n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breast data\n",
    "breast_data = pd.read_csv(\"data/breast_data_full.csv\")\n",
    "breast_data = breast_data.drop([\"id\"], axis=1)\n",
    "breast_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-tournament",
   "metadata": {},
   "source": [
    "We have a total of 30 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-federation",
   "metadata": {},
   "source": [
    "You start by Standardizing the data since PCA's output is influenced based on the scale of the features of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = breast_data.drop(\"diagnosis\", axis=1).columns.tolist()\n",
    "\n",
    "X = breast_data.loc[:, features].values\n",
    "X = StandardScaler().fit_transform(x) # normalizing the features\n",
    "\n",
    "# putting scalled data into dataframe format\n",
    "normalised_data = pd.DataFrame(X,columns=features)\n",
    "normalised_data[\"diagnosis\"] = diagnosis\n",
    "normalised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X),np.std(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-missouri",
   "metadata": {},
   "source": [
    "## PCA - Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-depression",
   "metadata": {},
   "source": [
    "When we have a dataset with a lot features, there usually are lots of features that are either redundant or noisy. For dimensionality reduction, it is important to identify these types of features, otherwise we might end up removing important information about our data.\n",
    "\n",
    "**In other words, we should be removing features that will not impact the patterns in data or the prediction results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-wagner",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is an unsupervised learning method that can help us reduce the dimensionlity of the space without impacting too much on the prediction results. PCA is especially important for a problem called *The Curse of Dimensionality* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-notice",
   "metadata": {},
   "source": [
    "### The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-gallery",
   "metadata": {},
   "source": [
    "The Curse of Dimensionality refers to various phenomena that arise when analysing and organising data in high-dimensional spaces, but does not occur in low-dimensional settings. Due to the curse of dimensionality, search algorithms suffer from an exponential decrease of performance as the dimension of the metric space increases. Here are two examples of the problem of the curse of dimensinality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-associate",
   "metadata": {},
   "source": [
    "<img src=\"./graphics/cd2.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-question",
   "metadata": {},
   "source": [
    "<img src=\"./graphics/cd1.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-inquiry",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = normalised_data.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    f, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax = sns.heatmap(corr, mask=mask, vmax=1.0, square=True, cmap=\"YlGnBu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-assets",
   "metadata": {},
   "source": [
    "Things to take into consideration when doing a correlation analysis:\n",
    "- We prefer variables that are highly correlated with the class variable (in this case, with \"diagnosis\");\n",
    "- We want to avoid input variables that are correlated with each other. This means that the variables share similar characteristics and for that reason are redundant. In this case, we should consider removing variables with a correlation higher than 0.6.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-burner",
   "metadata": {},
   "source": [
    "### A Method to Search for Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "df = normalised_data.drop(\"diagnosis\", axis=1)\n",
    "model = RandomForestRegressor(random_state=1, max_depth=10)\n",
    "df=pd.get_dummies(df)\n",
    "model.fit(df, normalised_data[\"diagnosis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 10\n",
    "\n",
    "features = normalised_data.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances[0:NUM_FEATURES])  # top features\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-drawing",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from statsmodels.multivariate.pca import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA( ) # you can specify a number of principal components using n_components\n",
    "pca_result = pca.fit_transform( normalised_data.drop(\"diagnosis\", axis=1)   )\n",
    "pd.DataFrame(pca_result) # principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(len(pca.explained_variance_ratio_))\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.title('Principal Component Analysis')\n",
    "plt.bar(index, pca.explained_variance_ratio_*100)\n",
    "plt.xlabel('Principal Component', fontsize=10)\n",
    "plt.ylabel('Explained Variance', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-commissioner",
   "metadata": {},
   "source": [
    "#### Method 1: Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we look at the eigen values and we discard all the components with eigenvalues smaller than 1\n",
    "# this is known as the Kaiser criterion. \n",
    "\n",
    "# in this Python library, the closest we get to the eigen values is the singular values\n",
    "pca.singular_values_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-regression",
   "metadata": {},
   "source": [
    "We can remove three features from this data according to Kaiser criterion. Maybe this is not enough, so let's look at another way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-threshold",
   "metadata": {},
   "source": [
    "#### Method 2: Explanation Variance\n",
    "\n",
    "The explanation variance is a methd that allows you to pick the total amount of variance in the dataset you want principal components to encode. Usually these cut-off values are 80% or 90%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cummulative variance can allow us to choose the components with the biggest variance\n",
    "cummulative_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "cumulative_variance_explained = pd.DataFrame(data=cummulative_var, columns=['cumulative_var'])\n",
    "print(cumulative_variance_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-agent",
   "metadata": {},
   "source": [
    "If we were to use this method, we have:\n",
    "- The first four principal components explain a bit more than 80% of the variance,\n",
    "- The first six principal components would roughly explain 90% of the variance in our dataset.\n",
    "\n",
    "We can use the first four or first six principal components, depending on the cut-off that we established. For this tutorial, let's use the first four. One can see this clearer in the following visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(range(len(pca.components_)), pca.explained_variance_ratio_)\n",
    "plt.plot(range(len(pca.components_)), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"PCA - Cumulative Explained Variance vs. Component-Explained Variance \")\n",
    "plt.legend((\"Component - Explained Variance\",\"Cumulative Sum - Explained Variance\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-skirt",
   "metadata": {},
   "source": [
    "#### Finalising Analysis\n",
    "\n",
    "We saw that 4 components are enough to express 80% of our data, so let's apply PCA for 4 compoenents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA( n_components = 4 )  \n",
    "pca_result = pca.fit_transform( normalised_data.drop(\"diagnosis\", axis=1)   )\n",
    "df_res = pd.DataFrame(pca_result, columns=[\"principal_component_1\",\"principal_component_2\",\"principal_component_3\",\"principal_component_4\"]) # principle components\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-triple",
   "metadata": {},
   "source": [
    "Remember in PCA we lose interpretability. By projecting the data into the component with the highest variance, you lose information and consequently interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretability\n",
    "import statsmodels.multivariate.pca as st\n",
    "\n",
    "pca2 = st.PCA( normalised_data.drop(\"diagnosis\", axis=1) , ncomp = 4, method='eig' )\n",
    "loadings = pca2.loadings\n",
    "loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-commissioner",
   "metadata": {},
   "source": [
    "### PCA on the Web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-graduation",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"./graphics/pca.png\" width=\"700\" />\n",
    "\n",
    "URL here: <a href=\"https://setosa.io/ev/principal-component-analysis/\">PCA example</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-examination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
