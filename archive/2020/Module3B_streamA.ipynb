{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [1] Explainable AI: providing users the *right* to explanation\n",
    "\n",
    "1. Goal\n",
    "2. Why we need it?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [1.1] Motiation\n",
    "\n",
    "Advanced machine learning approaches like neural networks lack transparency in their internal predictive mechnisms. Given a single instance, for example a patient, it is hard to understand why the machine learning algorithm is making a specific prediction about that specific patient. This generates the following problems:\n",
    "\n",
    "- Users cannot understand **why** a certain prediction was computed\n",
    "- Generates **lack of trust** in the model\n",
    "- Reluctance to use the predictions\n",
    "- Consequences of a misclassication / prediction\n",
    "- The model can be **biased**\n",
    "- The model can put certain societal groups at risk due to **discrimination**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [1.2] Explainable AI\n",
    "\n",
    "Explainable AI (XAI) is an emerging field in machine learning that aims to address how black box decisions of AI systems are made. This area inspects and tries to understand the steps and models involved in making decisions.\n",
    "\n",
    "<img src=\"https://www.darpa.mil/ddm_gallery/xai-figure2-inline-graphic.png\" width=90%/>\n",
    "\n",
    "Source: <a href=\"https://www.darpa.mil/program/explainable-artificial-intelligence\">DARPA: Defense Advanced Research Project Agency</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [1.3] Interpretability vs. Accuracy\n",
    "\n",
    "The current state of the art in machine learning techniques shows that the higher the accuracies a model can achieve, the more opaque it is to the user and consequently the less interpretable it is. \n",
    "\n",
    "<img src=\"images/XAI.jpg\" />\n",
    "\n",
    "Is a performance measure like accuracy enough to assess the quality of an analysis using machine learning techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [2] About High Accuracies...\n",
    "\n",
    "Consider the following scenario. A machine learning algorithm was used to distinguish between wolves and Huskys (which is a specific dog breed). The model achieves **extremely high accuracies: 90\\%**! When testing the model, it wrongly classifies the following image.\n",
    "\n",
    "<img src=\"images/husky.png\" width=25% />\n",
    "\n",
    "\n",
    "Why do you think the algorithm classified this as a wolf?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [2] About High Accuracies...\n",
    "\n",
    "The problem is in the training set. The wolves used in the training set were always in the snow, while dogs were not!\n",
    "\n",
    "This is the explanation given about the model's prediction:\n",
    "\n",
    "<img src=\"images/husky_explanation.png\" width=50% />\n",
    "\n",
    "Do you think high model accuracies are enough to assess the quality of a machine learning algorithm in data analytics? How can one trust a prediction from an autonomous system?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [3] Racial Biases: a Real World Case Study\n",
    "\n",
    "There was a software used across the USA to predict the probability of future recetivism in criminals. And it’s biased against black people...\n",
    "\n",
    "Analysis conducted by ProPublica: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [3.1] Case 1: Two Petty Arrests\n",
    "\n",
    "<img src=\"images/pro1.png\" width=70% />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [3.2] Case 2: Two Drug Possion Arrests\n",
    "\n",
    "<img src=\"images/pro2.png\" width=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [3.3] Case 3: Two DUI Arrests\n",
    "\n",
    "<img src=\"images/pro4.png\" width=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [3.4] Case 4: Two Shoplifting Arrests\n",
    "\n",
    "<img src=\"images/pro5.png\" width=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [3.5] Racial Bias: white defendants vs black defendats\n",
    "\n",
    "After ProPublica conducted an analysis of the proprietary software they found that white defendat's risk scores were skewed towards low scores while black defandants are uniform distributed.\n",
    "\n",
    "<img src=\"images/pro6.png\" width=100% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "When we remove some datapoints from our dataset, discard some features for our analysis, etc, we are imposing *our own viwes and ideoligies* in the data. Our own values and desires influence our choices starting in the data that we choose to collect to the questions we ask.\n",
    "\n",
    "Data analytics models are nothing more than our opinions and biases embedded in mathematics!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [4] Amazon Facial Recognition System\n",
    "\n",
    "Source: https://www.theguardian.com/technology/2018/jul/26/amazon-facial-rekognition-congress-mugshots-aclu\n",
    "\n",
    "Amazon’s facial recognition technology falsely identified 28 members of Congress as people who have been arrested for crimes, according to the American Civil Liberties Union (ACLU).\n",
    "\n",
    "<img src=\"https://i.guim.co.uk/img/media/cbfe22714f4b5aee97144f101aa01018c3c0db43/0_204_4096_2458/master/4096.jpg?width=620&quality=45&auto=format&fit=max&dpr=2&s=4e0e218a33e3fcdc9f2610067456ad4b\" width=50%/>\n",
    "\n",
    "The ACLU of Northern California’s test of Amazon’s controversial Rekognition software also found that **people of color were disproportionately misidentified in a mugshot database**, raising new concerns about racial bias and the potential for abuse by law enforcement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [5] Facial Recognition: Microsoft, Facebook and IBM\n",
    "\n",
    "Source: http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf\n",
    "\n",
    "An approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups was made. \n",
    "\n",
    "Three commercial gender classification systems were evaluated using a dataset balanced in terms of gender and skin colour. The following image shows an example of the dataset used for evaluation:\n",
    "\n",
    "<img src=\"images/gender_shades2.png\" width=50% />\n",
    "\n",
    "Results show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The\n",
    "\n",
    "<img src=\"images/gender_shades1.png\" width=70% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [6]  Discussion: How to decrease the risk of racial bias and gender bias in data analytics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
