{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca249a0-2ebd-4c90-91b6-16d6351ad606",
   "metadata": {
    "cell_name": "header_cell"
   },
   "source": [
    "<div style=\"background:#E9FFF6; color:#440404; padding:8px; border-radius: 4px; text-align: center; font-weight: 500;\">IFN619 - Data Analytics for Strategic Decision Makers</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6489c-6c2c-4aec-8962-63884b4984ed",
   "metadata": {
    "cell_name": "title_cell"
   },
   "source": [
    "# IFN619 :: C1-UnstructuredAnalytics\n",
    "\n",
    "For this tutorial, you will use the studio notebook as a guide, and:\n",
    "\n",
    "1. Use the Guardian API to undertake your own search and obtain a json file of documents\n",
    "2. Create a TF/IDF document-term matrix for your documents\n",
    "3. Perform topic modelling of your documents using NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import pandas as pd\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3afe99",
   "metadata": {},
   "source": [
    "### 1. Accessing the data via The Guardian API\n",
    "\n",
    "Make a copy of the lecture notebook file (Accessing the Guardian API), and modify it to perform your own search of the Guardian API. **NOTE:** you will need to obtain your own developer API key first and put it in a file in the appropriate folder.\n",
    "\n",
    "A suggested search term is \"cyclone alfred\", or come up with another that is of interest to you and will return a fair amount of data.\n",
    "\n",
    "Save your search results in a json file, then read in that data below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a95d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data - articles from The Guardian\n",
    "file_path = \"data/\"\n",
    "file_name = \"???\"\n",
    "\n",
    "with open(f\"{file_path}{file_name}\",'r', encoding='utf-8') as fp:\n",
    "    articles = json.load(fp)\n",
    "\n",
    "print(f\"Loaded {len(articles)} articles from {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f23693-487b-468e-843e-ed3e65b70c17",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "Let's have a quick look what articles we have collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344308c-a0e9-4468-bce4-119b6b64c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An overview of article titles\n",
    "for title in articles.keys():\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d00f685-8f0d-433e-bec5-0915610b1c11",
   "metadata": {},
   "source": [
    "Did all the retrieved articles relevant to the topic? If not, what are the strategies we might use to filter articles that are relevant to our topic? \n",
    "- ???\n",
    "- ???\n",
    "- ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82120c70-77f8-4a66-91ff-bf7b1f51cff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implement some of your strategies - add more cells if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2378cc",
   "metadata": {},
   "source": [
    "#### Create a top10 terms dataframe\n",
    "\n",
    "Using the index from the documents, create a dataframe that can hold the top10 terms for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edcb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to hold top terms for each analysis type\n",
    "terms_df = pd.DataFrame(index=articles.keys(),columns=['tfidf','nmf'])\n",
    "terms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4663b5",
   "metadata": {},
   "source": [
    "### Term Frequency / Inverse Document Frequency (TF/IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters appropriate to your data\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=???, min_df=???, max_features=???, stop_words=\"english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56935be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the document vectors\n",
    "tfidf_dt_matrix = tfidf_vectorizer.fit_transform(???)\n",
    "\n",
    "# Display the vector for the first document\n",
    "tfidf_dt_matrix.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8893fd",
   "metadata": {},
   "source": [
    "#### Update the terms matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74abd3-aa2e-49e3-a35c-ec6f706b8753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of feature names\n",
    "feature_names = ???.get_feature_names_out()\n",
    "\n",
    "# create a df to combine matrix with feature names\n",
    "tfidf_df = pd.DataFrame(tfidf_dt_matrix.toarray(), index=???, columns=???)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6eb661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in terms_df.index:\n",
    "    tfidf = dict(tfidf_df.loc[idx].sort_values(ascending=False).head(???))\n",
    "    #print(counts)\n",
    "    terms_df.at[idx,'tfidf'] = list(tfidf.keys()) \n",
    "\n",
    "terms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ede50",
   "metadata": {},
   "source": [
    "### Topic modelling with Non-negative Matrix Factorisation (NMF)\n",
    "\n",
    "\n",
    "[NMF](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a different algorithm for obtaining *topics* (a list of terms) from a document-term matrix. It also factorises the document-term matrix into 2 factor matrices: document-topic and topic-term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of topics\n",
    "num_topics = ???\n",
    "\n",
    "# Create the model\n",
    "nmf_model = NMF(n_components=???,init='random',beta_loss='frobenius')\n",
    "\n",
    "# Fit the model to the data and use it to transform the data\n",
    "doc_topic_nmf = nmf_model.fit_transform(???)\n",
    "\n",
    "topic_term_nmf = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284934af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the topics and their terms\n",
    "nmf_topic_dict = {}\n",
    "for index, topic in enumerate(???):\n",
    "    zipped = zip(feature_names, topic)\n",
    "    top_terms=dict(sorted(zipped, key = lambda t: t[1], reverse=True)[:10])\n",
    "    #print(top_terms)\n",
    "    top_terms_list= {key : round(top_terms[key], 4) for key in top_terms.keys()}\n",
    "    nmf_topic_dict[f\"topic_{index}\"] = top_terms_list\n",
    "\n",
    "# Print the topics with their terms    \n",
    "for k,v in nmf_topic_dict.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9218b0",
   "metadata": {},
   "source": [
    "#### Update the terms matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c2c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx,topic in enumerate(???):\n",
    "    topic_num = topic.argmax()\n",
    "    top_topic = nmf_topic_dict[f\"topic_{topic_num}\"]\n",
    "    title = terms_df.index[idx]\n",
    "    terms_df.loc[title,'nmf'] = list(top_topic.keys())\n",
    "\n",
    "terms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aed516",
   "metadata": {},
   "source": [
    "### Check against articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359038ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 random articles\n",
    "samples = random.sample(range(0,len(terms_df)),5)\n",
    "\n",
    "for sample in samples:\n",
    "    doc = terms_df.iloc[sample]\n",
    "    print(f\"[{sample}] {doc.name}\")\n",
    "    print(\"\\t- TFIDF:\\t\",doc['tfidf'])\n",
    "    print(\"\\t- NMF:\\t\\t\",doc['nmf'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfeaac6",
   "metadata": {},
   "source": [
    "## Refine your analysis\n",
    "\n",
    "Once you have worked through the process. Try tweaking the parameters in the TF/IDF vectorizer and also in the NMF topic modelling to try and obtain better results for your data.\n",
    "\n",
    "#### Advanced\n",
    "\n",
    "You may obtain better results by doing the following:\n",
    "\n",
    "- Creating smaller documents (e.g. article paragraphs)\n",
    "- Pre-processing the text by Stemming or Lemmatizing, and by removing additional stop words.\n",
    "- ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d027f-ff80-47f3-a09d-5db7e85ef15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "creation_period": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "nb_name": "template",
  "qut": {
   "creation_period": "2023_sem1",
   "nb_name": "C1-UnstructuredAnalytics-tutorial",
   "unit_code": "IFN619"
  },
  "unit_code": ""
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
